# ML--Algo

## MY Machine Learning Journey 

``` Started my ML journey on the 1st of october . 
Have learnt a lot . Will start to give update on what i learn each day .
```
# ðŸ§® Logistic Regression â€” A Probabilistic Classifier

Logistic Regression is a **statistical model** used to estimate the **probability** of an event belonging to one of two categories (binary classification).

---

## 1ï¸âƒ£ Linear Equation

The model starts by calculating a **linear combination** of inputs, similar to linear regression:

![z](https://latex.codecogs.com/svg.image?\bg_white%20z=\mathbf{w}\cdot\mathbf{x}+b)

> ðŸ’¡ Here:
> - **w** â†’ weight vector  
> - **x** â†’ feature vector  
> - **b** â†’ bias term  
> - **z** â†’ raw model output (can range from âˆ’âˆž to +âˆž)

---

## 2ï¸âƒ£ The Sigmoid (Logistic) Function

To map the linear output `z` into a probability between 0 and 1, we pass it through the **Sigmoid function**:

![sigmoid](https://latex.codecogs.com/svg.image?\bg_white%20\sigma(z)=\frac{1}{1+e^{-z}})

> - If the output â‰¥ 0.5 â†’ predict **class 1**
> - Otherwise â†’ predict **class 0**

This is what makes Logistic Regression **probabilistic** â€” it gives a *likelihood* rather than a direct class.

---

## 3ï¸âƒ£ Machine Learning as Optimization

Training a machine learning model is an **optimization problem**.

We want to find the parameters (**w**, **b**) that **minimize the error** between predicted probabilities and actual labels.

This is done through **Gradient Descent**, which iteratively updates weights in the opposite direction of the gradient of the loss function.

---

## 4ï¸âƒ£ Loss Function â€” Binary Cross-Entropy (Log Loss)

Because Logistic Regression predicts probabilities, it cannot use Mean Squared Error (MSE).  
Instead, it uses the **Binary Cross-Entropy Loss**, also called **Log Loss**.

### ðŸ’¡ Concept

Cross-Entropy heavily penalizes confident wrong predictions â€”  
e.g., predicting 0.99 when the true label is 0 gives a large penalty.

### ðŸ“˜ Formula

For `m` training examples:

![J(w)](https://latex.codecogs.com/svg.image?\bg_white%20J(\mathbf{w})=-\frac{1}{m}\sum_{i=1}^{m}\left[y^{(i)}\log(h_{\mathbf{w}}(\mathbf{x}^{(i)}))+(1-y^{(i)})\log(1-h_{\mathbf{w}}(\mathbf{x}^{(i)}))\right])

where:

- ![y](https://latex.codecogs.com/svg.image?\bg_white%20y^{(i)}) â†’ true label (0 or 1)  
- ![h](https://latex.codecogs.com/svg.image?\bg_white%20h_{\mathbf{w}}(\mathbf{x}^{(i)})) â†’ predicted probability from the sigmoid

---

## 5ï¸âƒ£ Gradient Descent Updates

Weights are updated using the **gradient of the loss**:

![gradient](https://latex.codecogs.com/svg.image?\bg_white%20\mathbf{w}:=\mathbf{w}-\alpha\frac{\partial{J(\mathbf{w})}}{\partial{\mathbf{w}}})

> - **Î±** â†’ learning rate  
> - **âˆ‚J/âˆ‚w** â†’ derivative of the loss function w.r.t. weights  

This process repeats until the loss converges to its minimum.

---

### ðŸ§  Summary

| Concept       | Equation                                                                                                                                                                                                         | Meaning                     |
| ------------- | ---------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------- | --------------------------- |
| Linear output | ![z](https://latex.codecogs.com/svg.image?\bg_white%20z=\mathbf{w}\cdot\mathbf{x}+b)                                                                                                                             | Weighted sum of inputs      |
| Sigmoid       | ![sigmoid](https://latex.codecogs.com/svg.image?\bg_white%20\sigma(z)=\frac{1}{1+e^{-z}})                                                                                                                        | Converts to probability     |
| Loss          | ![loss](https://latex.codecogs.com/svg.image?\bg_white%20J(\mathbf{w})=-\frac{1}{m}\sum_{i=1}^{m}\left[y^{(i)}\log(h_{\mathbf{w}}(\mathbf{x}^{(i)}))+(1-y^{(i)})\log(1-h_{\mathbf{w}}(\mathbf{x}^{(i)}))\right]) | Penalizes wrong predictions |
| Update rule   | ![gradient](https://latex.codecogs.com/svg.image?\bg_white%20\mathbf{w}:=\mathbf{w}-\alpha\frac{\partial{J(\mathbf{w})}}{\partial{\mathbf{w}}})                                                                  | Weight optimization         |

---

âœ¨ *Logistic Regression bridges the gap between Linear Models and Probabilistic Classification â€” forming the foundation for Neural Networks and Deep Learning.*


