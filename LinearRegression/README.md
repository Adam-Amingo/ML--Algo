# üìà Linear Regression ‚Äî Predicting Continuous Outcomes

Linear Regression is a **supervised learning algorithm** used to model the **relationship between input variables (features)** and a **continuous output variable (target)**.  
It assumes a **linear relationship** between the inputs and the target.

---

## 1Ô∏è‚É£ The Hypothesis Function

In **matrix form**, the model predicts the output vector **≈∑** as:

![h](https://latex.codecogs.com/svg.image?\bg_white\hat{\mathbf{y}}=\mathbf{X}\mathbf{w}+\mathbf{b})

> üí° Where:
> - **X** ‚Üí matrix of input features (size: *m √ó n*)  
> - **w** ‚Üí column matrix of weights (size: *n √ó 1*)  
> - **b** ‚Üí bias term (scalar or broadcasted)  
> - **≈∑** ‚Üí predicted outputs (size: *m √ó 1*)  

---

## 2Ô∏è‚É£ Goal of Linear Regression

We want to find parameters **W** and **b** such that the predictions `≈∑` are **as close as possible** to the true outputs **y**.  

This means minimizing the **difference (error)** between predicted and actual values.

---

## 3Ô∏è‚É£ Cost Function ‚Äî Mean Squared Error (MSE)

In matrix notation, the **Mean Squared Error** (MSE) is given by:

![J](https://latex.codecogs.com/svg.image?\bg_white J(\mathbf{w},b)=\frac{1}{2m}(\mathbf{X}\mathbf{w}+\mathbf{b}-\mathbf{y})^T(\mathbf{X}\mathbf{w}+\mathbf{b}-\mathbf{y}))

> üí° Here:
> - **m** ‚Üí number of training examples  
> - **y** ‚Üí actual output matrix (*m √ó 1*)  
> - **≈∑ = Xw + b** ‚Üí predicted output matrix (*m √ó 1*)  

The factor `1/2m` simplifies derivative computation.

---

## 4Ô∏è‚É£ Gradient Descent ‚Äî Learning the Best Parameters

To minimize the cost, we use **Gradient Descent**, which updates the parameters iteratively:

![update](https://latex.codecogs.com/svg.image?\bg_white\mathbf{w}:=\mathbf{w}-\alpha\frac{1}{m}\mathbf{X}^T(\mathbf{X}\mathbf{w}+\mathbf{b}-\mathbf{y}))

![bupdate](https://latex.codecogs.com/svg.image?\bg_white b:=b-\alpha\frac{1}{m}\sum_{i=1}^{m}(\hat{y}^{(i)}-y^{(i)}))

> - **Œ±** ‚Üí learning rate  
> - Updates continue until convergence (minimum cost)

---

## 5Ô∏è‚É£ Ordinary Least Squares (OLS) ‚Äî The Analytical Solution

Instead of iterative updates, **OLS** gives a **closed-form solution** for the optimal weights that minimize the cost function.

### üìò Formula

![ols](https://latex.codecogs.com/svg.image?\bg_white\mathbf{w}=(\mathbf{X}^T\mathbf{X})^{-1}\mathbf{X}^T\mathbf{y})

> üí° Notes:
> - **X** ‚Üí matrix of input features  
> - **y** ‚Üí column matrix of target values  
> - **w** ‚Üí matrix of optimal weights  

This solution directly minimizes the **sum of squared errors**, giving the **best-fit regression plane (or line)**.

---

## 6Ô∏è‚É£ The Regression Line (Simple Case)

For a single feature (simple linear regression), the hypothesis simplifies to:

![line](https://latex.codecogs.com/svg.image?\bg_white\hat{y}=w_1x+b)

> - `w‚ÇÅ` ‚Üí slope of the line  
> - `b` ‚Üí intercept (value of y when x = 0)

---

### üß† Summary

| Concept          | Equation                                                                                                                                                                    | Meaning                              |
| ---------------- | --------------------------------------------------------------------------------------------------------------------------------------------------------------------------- | ------------------------------------ |
| Hypothesis       | ![h](https://latex.codecogs.com/svg.image?\bg_white\hat{\mathbf{y}}=\mathbf{X}\mathbf{w}+\mathbf{b})                                                                        | Predicts output vector               |
| Cost Function    | ![J](https://latex.codecogs.com/svg.image?\bg_white J(\mathbf{w},b)=\frac{1}{2m}(\mathbf{X}\mathbf{w}+\mathbf{b}-\mathbf{y})^T(\mathbf{X}\mathbf{w}+\mathbf{b}-\mathbf{y})) | Measures prediction error            |
| Gradient Descent | ![update](https://latex.codecogs.com/svg.image?\bg_white\mathbf{w}:=\mathbf{w}-\alpha\frac{1}{m}\mathbf{X}^T(\mathbf{X}\mathbf{w}+\mathbf{b}-\mathbf{y}))                   | Iterative optimization               |
| OLS Solution     | ![ols](https://latex.codecogs.com/svg.image?\bg_white\mathbf{w}=(\mathbf{X}^T\mathbf{X})^{-1}\mathbf{X}^T\mathbf{y})                                                        | Analytical solution (direct weights) |

---

‚ú® *Linear Regression (in matrix form) is the cornerstone of statistical learning ‚Äî simple, interpretable, and the foundation for more complex models like Ridge and Lasso Regression.*



```








```






# üìä Multivariate Linear Regression ‚Äî (4 Features, Matrix Form)

Linear Regression models the relationship between **multiple input features** and a **continuous output variable**.  
It assumes the output is a **linear combination** of all input features.

---

## 1Ô∏è‚É£ Model Representation

For a dataset with **4 features**, the hypothesis in **matrix form** is:

![h](https://latex.codecogs.com/svg.image?\bg_white\hat{\mathbf{y}}=\mathbf{X}\mathbf{w}+\mathbf{b})

> üí° Where:
> - **X** ‚Üí Input matrix of size *(m √ó 4)*  
>   \[
>   \mathbf{X} = 
>   \begin{bmatrix}
>   x_{11} & x_{12} & x_{13} & x_{14} \\
>   x_{21} & x_{22} & x_{23} & x_{24} \\
>   \vdots & \vdots & \vdots & \vdots \\
>   x_{m1} & x_{m2} & x_{m3} & x_{m4}
>   \end{bmatrix}
>   \]
> - **w** ‚Üí Column matrix of weights *(4 √ó 1)*  
>   \[
>   \mathbf{w}=
>   \begin{bmatrix}
>   w_1\\
>   w_2\\
>   w_3\\
>   w_4
>   \end{bmatrix}
>   \]
> - **b** ‚Üí Bias term (scalar, added to all predictions)  
> - **≈∑** ‚Üí Predicted outputs *(m √ó 1)*  

So each prediction is computed as:

![yhat](https://latex.codecogs.com/svg.image?\bg_white\hat{y}^{(i)}=w_1x_1^{(i)}+w_2x_2^{(i)}+w_3x_3^{(i)}+w_4x_4^{(i)}+b)

---

## 2Ô∏è‚É£ Cost Function ‚Äî Mean Squared Error (MSE)

The cost function measures the average squared difference between the predicted and actual values:

![J](https://latex.codecogs.com/svg.image?\bg_white J(\mathbf{w},b)=\frac{1}{2m}(\mathbf{X}\mathbf{w}+\mathbf{b}-\mathbf{y})^T(\mathbf{X}\mathbf{w}+\mathbf{b}-\mathbf{y}))

> üí° Intuition:
> - Penalizes large deviations between predictions and actual labels.
> - The goal is to minimize this cost.

---

## 3Ô∏è‚É£ Gradient Descent ‚Äî Iterative Optimization

To minimize the cost function, we update parameters iteratively:

**Weight update rule:**
![update](https://latex.codecogs.com/svg.image?\bg_white\mathbf{w}:=\mathbf{w}-\alpha\frac{1}{m}\mathbf{X}^T(\mathbf{X}\mathbf{w}+\mathbf{b}-\mathbf{y}))

**Bias update rule:**
![bupdate](https://latex.codecogs.com/svg.image?\bg_white b:=b-\alpha\frac{1}{m}\sum_{i=1}^{m}(\hat{y}^{(i)}-y^{(i)}))

> - **Œ±** ‚Üí learning rate  
> - Continue until the cost function converges to its minimum.

---

## 4Ô∏è‚É£ Ordinary Least Squares (OLS) ‚Äî Closed Form Solution

OLS provides a **direct analytical solution** (no iteration) to find the optimal weights:

![ols](https://latex.codecogs.com/svg.image?\bg_white\mathbf{w}=(\mathbf{X}^T\mathbf{X})^{-1}\mathbf{X}^T\mathbf{y})

Once **w** is computed, you can find **b** as:

![b](https://latex.codecogs.com/svg.image?\bg_white b=\bar{y}-\bar{\mathbf{X}}\mathbf{w})

> üí° Where:
> - **X** ‚Üí input matrix (*m √ó 4*)  
> - **y** ‚Üí target column (*m √ó 1*)  
> - **w** ‚Üí learned weight column (*4 √ó 1*)  
> - **b** ‚Üí scalar bias (intercept)

---

## 5Ô∏è‚É£ Example ‚Äî Predicting House Prices üè†

Let‚Äôs say we have 4 features:

| Feature | Description               |
| ------- | ------------------------- |
| x‚ÇÅ      | Number of rooms           |
| x‚ÇÇ      | Size (in square meters)   |
| x‚ÇÉ      | Distance from city center |
| x‚ÇÑ      | Age of house              |

The model becomes:

![example](https://latex.codecogs.com/svg.image?\bg_white\hat{y}=w_1x_1+w_2x_2+w_3x_3+w_4x_4+b)

The coefficients (**w‚ÇÅ‚Äìw‚ÇÑ**) represent how strongly each feature influences the predicted price.

---

## üß† Summary

| Concept          | Equation                                                                                                                                                                    | Meaning                           |
| ---------------- | --------------------------------------------------------------------------------------------------------------------------------------------------------------------------- | --------------------------------- |
| Hypothesis       | ![h](https://latex.codecogs.com/svg.image?\bg_white\hat{\mathbf{y}}=\mathbf{X}\mathbf{w}+\mathbf{b})                                                                        | Predicts output vector            |
| Cost Function    | ![J](https://latex.codecogs.com/svg.image?\bg_white J(\mathbf{w},b)=\frac{1}{2m}(\mathbf{X}\mathbf{w}+\mathbf{b}-\mathbf{y})^T(\mathbf{X}\mathbf{w}+\mathbf{b}-\mathbf{y})) | Measures prediction error         |
| Gradient Descent | ![update](https://latex.codecogs.com/svg.image?\bg_white\mathbf{w}:=\mathbf{w}-\alpha\frac{1}{m}\mathbf{X}^T(\mathbf{X}\mathbf{w}+\mathbf{b}-\mathbf{y}))                   | Iteratively finds minimum loss    |
| OLS Solution     | ![ols](https://latex.codecogs.com/svg.image?\bg_white\mathbf{w}=(\mathbf{X}^T\mathbf{X})^{-1}\mathbf{X}^T\mathbf{y})                                                        | Analytical least-squares solution |

---

‚ú® *Linear Regression with 4 features generalizes easily to any number of predictors ‚Äî forming the mathematical foundation for Ridge, Lasso, and even Neural Network layers.*


# ‚öñÔ∏è Ordinary Least Squares (OLS) vs Ridge Regression

Both **OLS** and **Ridge Regression** are linear models used to estimate coefficients \( \beta \) that best fit the relationship between features \( X \) and target \( y \).  
The main difference lies in how they handle **model complexity** and **overfitting**.

---

```


```

## 1Ô∏è‚É£ Ordinary Least Squares (OLS)

OLS minimizes the **sum of squared errors** between predicted and actual values.

### üìò Objective Function

![ols_obj](https://latex.codecogs.com/svg.image?\bg_white J(\beta)=\frac{1}{2m}\sum_{i=1}^{m}(\hat{y}^{(i)}-y^{(i)})^{2})

or in **matrix form**:

![ols_matrix](https://latex.codecogs.com/svg.image?\bg_white J(\beta)=\frac{1}{2m}(X\beta-y)^{T}(X\beta-y))

### üßÆ Closed-Form Solution

OLS provides an analytical solution for the optimal parameters:

![ols_solution](https://latex.codecogs.com/svg.image?\bg_white\hat{\beta}_{OLS}=(X^{T}X)^{-1}X^{T}y)

> üí° OLS assumes:
> - No multicollinearity among features  
> - Homoscedasticity (constant variance)  
> - Independence of errors  

---

## 2Ô∏è‚É£ Ridge Regression (L2 Regularization)

Ridge Regression extends OLS by adding a **penalty term** on the magnitude of coefficients.

### üìò Objective Function

![ridge_obj](https://latex.codecogs.com/svg.image?\bg_whiteJ(\beta)=\frac{1}{2m}(X\beta-y)^{T}(X\beta-y)+\lambda\|\beta\|_{2}^{2})

> - \( \lambda \geq 0 \): regularization strength  
> - \( \|\beta\|_{2}^{2} = \sum_{j=1}^{n}\beta_{j}^{2} \): L2 norm  

### üßÆ Closed-Form Solution

The Ridge solution modifies OLS by adding \( \lambda I \) to stabilize inversion:

![ridge_solution](https://latex.codecogs.com/svg.image?\bg_white\hat{\beta}_{Ridge}=(X^{T}X+\lambdaI)^{-1}X^{T}y)

> üîç When \( \lambda = 0 \), Ridge Regression becomes identical to OLS.  
> As \( \lambda \) increases, coefficients shrink toward zero, reducing overfitting.

---

## 3Ô∏è‚É£ Comparison Summary

| Concept                 | OLS                                                                                                   | Ridge Regression                                                                                                                 |
| ----------------------- | ----------------------------------------------------------------------------------------------------- | -------------------------------------------------------------------------------------------------------------------------------- |
| **Objective**           | ![ols_J](https://latex.codecogs.com/svg.image?\bg_whiteJ(\beta)=\frac{1}{2m}(X\beta-y)^{T}(X\beta-y)) | ![ridge_J](https://latex.codecogs.com/svg.image?\bg_whiteJ(\beta)=\frac{1}{2m}(X\beta-y)^{T}(X\beta-y)+\lambda\|\beta\|_{2}^{2}) |
| **Solution**            | ![ols_sol](https://latex.codecogs.com/svg.image?\bg_white\hat{\beta}_{OLS}=(X^{T}X)^{-1}X^{T}y)       | ![ridge_sol](https://latex.codecogs.com/svg.image?\bg_white\hat{\beta}_{Ridge}=(X^{T}X+\lambdaI)^{-1}X^{T}y)                     |
| **Regularization Term** | None                                                                                                  | \( \lambda\|\beta\|_{2}^{2} \)                                                                                                   |
| **Effect of Œª**         | ‚Äî                                                                                                     | Controls coefficient shrinkage                                                                                                   |
| **Overfitting**         | Prone when multicollinearity exists                                                                   | Reduces overfitting and variance                                                                                                 |
| **Interpretability**    | Easier                                                                                                | Slightly less intuitive (coefficients biased toward 0)                                                                           |

---

## üß† Key Takeaways

- **OLS** finds parameters that minimize residual error ‚Äî perfect when features are independent.  
- **Ridge Regression** adds an L2 penalty to improve generalization and numerical stability.  
- Ridge helps when \( X^T X \) is nearly singular or features are highly correlated.

---

‚ú® *Ridge Regression = OLS + Penalty Term ‚Üí More robust, less variance, smoother generalization.*

