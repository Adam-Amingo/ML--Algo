# Gradient Descent and Stochastic Gradient Descent (SGD)

## 1Ô∏è‚É£ Overview

Gradient Descent is an optimization algorithm used to minimize a **cost function** by iteratively moving in the direction of the steepest descent as defined by the negative of the gradient. It‚Äôs a cornerstone method for training **machine learning models** such as linear regression and logistic regression.

---

## 2Ô∏è‚É£ The General Idea

The goal is to minimize the cost function:

![J(Œ∏)](https://latex.codecogs.com/svg.image?J(\theta)=\frac{1}{2m}\sum_{i=1}^{m}(h_{\theta}(x^{(i)})-y^{(i)})^{2})

where:
- \( m \) ‚Äî number of training examples  
- \( h_{\theta}(x^{(i)}) \) ‚Äî predicted value  
- \( y^{(i)} \) ‚Äî actual value  

The parameter update rule is:

![theta update](https://latex.codecogs.com/svg.image?\theta_j:=\theta_j-\alpha\frac{\partial}{\partial\theta_j}J(\theta))

where:
- \( \alpha \) ‚Äî learning rate  
- \( \frac{\partial}{\partial\theta_j}J(\theta) \) ‚Äî gradient of the cost function with respect to parameter \( \theta_j \)

---

## 3Ô∏è‚É£ Batch Gradient Descent (BGD)

### Equation

In **Batch Gradient Descent**, the update for each parameter uses **all training examples**:

![BGD equation](https://latex.codecogs.com/svg.image?\theta_j:=\theta_j-\alpha\frac{1}{m}\sum_{i=1}^{m}(h_{\theta}(x^{(i)})-y^{(i)})x_j^{(i)})

### When to Use
- When the dataset is **small to medium-sized** and can fit in memory.
- When **accurate gradient estimates** are needed.

### Advantages
‚úÖ Converges smoothly to the global minimum for convex functions.  
‚úÖ More stable updates ‚Äî less noise in the learning process.

### Disadvantages
‚ùå Slow for large datasets ‚Äî every update requires a full pass through all data.  
‚ùå Computationally expensive.

---

## 4Ô∏è‚É£ Stochastic Gradient Descent (SGD)

### Equation

In **Stochastic Gradient Descent**, the update happens **after every training example**:

![SGD equation](https://latex.codecogs.com/svg.image?\theta_j:=\theta_j-\alpha(h_{\theta}(x^{(i)})-y^{(i)})x_j^{(i)})

### When to Use
- When working with **very large datasets** or **streaming data**.
- When quick convergence or **online learning** is needed.

### Advantages
‚úÖ Much faster for large datasets.  
‚úÖ Can escape local minima due to noisy updates.  
‚úÖ Suitable for real-time and online learning.

### Disadvantages
‚ùå The loss function fluctuates (noisy convergence).  
‚ùå May overshoot the minimum if the learning rate is not well-tuned.

---

## 5Ô∏è‚É£ Comparison Table

| Feature              | Batch Gradient Descent | Stochastic Gradient Descent    |
| -------------------- | ---------------------- | ------------------------------ |
| Update per iteration | Uses all samples       | Uses one sample                |
| Speed                | Slow on large data     | Fast, efficient                |
| Stability            | Stable convergence     | Noisy convergence              |
| Memory use           | High                   | Low                            |
| Ideal for            | Small datasets         | Large datasets, streaming data |

---

## 6Ô∏è‚É£ Summary Visualization

Gradient Descent can be visualized as moving downhill on a cost surface ‚Äî the learning rate \( \alpha \) determines the size of each step:

![GD surface](https://latex.codecogs.com/svg.image?\theta_{new}=\theta_{old}-\alpha\nabla_{\theta}J(\theta))

---

## üß† Key Takeaway

- **Batch Gradient Descent** ‚Üí precise but slow.  
- **Stochastic Gradient Descent** ‚Üí fast but noisy.  
- **Mini-batch Gradient Descent** (not covered here) offers a balance between both.

---
